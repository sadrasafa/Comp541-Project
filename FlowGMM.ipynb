{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "julia-1.3",
      "display_name": "Julia 1.3"
    },
    "accelerator": "GPU",
    "colab": {
      "name": "FlowGMM.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sadrasafa/Comp541-Project/blob/master/FlowGMM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q7S9cpFJqfXy",
        "colab_type": "text"
      },
      "source": [
        "## Julia on Colaboratory ##\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BrHjOFFsxf7W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Installation cell\n",
        "%%shell\n",
        "if ! command -v julia 2>&1 > /dev/null\n",
        "then\n",
        "    wget 'https://julialang-s3.julialang.org/bin/linux/x64/1.3/julia-1.3.1-linux-x86_64.tar.gz' \\\n",
        "        -O /tmp/julia.tar.gz\n",
        "    tar -x -f /tmp/julia.tar.gz -C /usr/local --strip-components 1\n",
        "    rm /tmp/julia.tar.gz\n",
        "fi\n",
        "julia -e 'using Pkg; pkg\"add Plots; add PyPlot; add IJulia; add Knet; precompile\"'\n",
        "julia -e 'using Pkg; pkg\"build Knet;\"'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1VdFDb9szt_o",
        "colab_type": "code",
        "outputId": "7966241c-f3b8-4b4c-b76d-0626886e0aa4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        }
      },
      "source": [
        "using Knet\n",
        "# Test if Knet is using gpu\n",
        "Knet.gpu()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "┌ Info: Precompiling Knet [1902f260-5fb4-5aff-8c31-6271790ab950]\n",
            "└ @ Base loading.jl:1273\n",
            "┌ Warning: You are using CUDNN 7.6.5 for CUDA 10.1.0 with CUDA toolkit 10.0.130; these might be incompatible.\n",
            "└ @ CuArrays /root/.julia/packages/CuArrays/HE8G6/src/CuArrays.jl:127\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tDF_WvugvoJn",
        "colab_type": "text"
      },
      "source": [
        "# Install Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bxbZJq4SuKvT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "using Pkg;\n",
        "\n",
        "# Install missing packages\n",
        "for p in [\"Knet\", \"MLJ\", \"MLJModels\", \"Distributions\", \"Plots\", \"PyPlot\"]\n",
        "    if !haskey(Pkg.installed(),p)\n",
        "        Pkg.add(p);\n",
        "    end\n",
        "end\n",
        "\n",
        "using Knet, Plots, Random, PyPlot, MLJ, Distributions, LinearAlgebra"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VmYkco-0Y6Bb",
        "colab_type": "text"
      },
      "source": [
        "# FlowGMM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8cDqJrdTNX8e",
        "colab_type": "code",
        "outputId": "f9484032-1f8d-48d0-e452-aeae5ba7db57",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "atype = (Knet.gpu()>=0 ? Knet.KnetArray{Float32} : Array{Float32})"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "KnetArray{Float32,N} where N"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zM-GrorcY7Ib",
        "colab_type": "code",
        "outputId": "c5bcd689-3206-4ed8-b75e-3759a2e05895",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "struct Mask; d; reverse; end\n",
        "\n",
        "#one argument: (mask) will return x_id and x_change\n",
        "#two argument: (unmask) will_return concat(y_id, y_change)\n",
        "function (mask::Mask)(x) \n",
        "    len = size(x, 1)\n",
        "    b = convert(atype,zeros(len,1))\n",
        "    d = mask.d\n",
        "    if mask.reverse \n",
        "        b[d+1:end,1] .= 1\n",
        "    else\n",
        "        b[1:d,1] .= 1\n",
        "    end\n",
        "    x_id = x .* b\n",
        "    x_change = x .* (1 .- b)\n",
        "    return x_id, x_change\n",
        "end\n",
        "function (mask::Mask)(y_id, y_change)\n",
        "    len = size(y_id, 1)\n",
        "    b = convert(atype,zeros(len,1))\n",
        "    d = mask.d\n",
        "    if mask.reverse \n",
        "        b[d+1:end,1] .= 1\n",
        "    else\n",
        "        b[1:d,1] .= 1\n",
        "    end\n",
        "    return y_id .* b + y_change .* (1 .- b)\n",
        "end\n",
        "\n",
        "\n",
        "struct Sequential\n",
        "    layers\n",
        "    Sequential(layers...) = new(layers)\n",
        "end\n",
        "(s::Sequential)(x) = (for l in s.layers; x = l(x); end; x)\n",
        "\n",
        "struct DenseLayer; w; b; f; end\n",
        "\n",
        "DenseLayer(i::Int,o::Int, f=relu) = DenseLayer(param(o,i), param0(o), f)\n",
        "\n",
        "(d::DenseLayer)(x) = d.f.(d.w * x .+ d.b)\n",
        "\n",
        "\n",
        "\n",
        "#Coupling Layer\n",
        "mutable struct CouplingLayer; st_net::Sequential; mask::Mask; logdet; end\n",
        "\n",
        "function CouplingLayer(;in_dim::Int, hidden_dim::Int, num_layers::Int, mask::Mask)\n",
        "    layers = []\n",
        "    push!(layers, DenseLayer(in_dim, hidden_dim, relu))\n",
        "    for layer in 1:num_layers\n",
        "        push!(layers, DenseLayer(hidden_dim, hidden_dim, relu))\n",
        "    end\n",
        "    push!(layers, DenseLayer(hidden_dim, 2*in_dim, identity))\n",
        "    st_net = Sequential(layers...)\n",
        "    CouplingLayer(st_net, mask, 0.0)\n",
        "end\n",
        "\n",
        "function (cpl::CouplingLayer)(x)\n",
        "    x_id, x_change, s, t = get_s_and_t(cpl, x)\n",
        "    y_change = x_change .* exp.(s) .+ t #in original code, first addition is performed, then exponentiation\n",
        "    y_id = x_id\n",
        "    cpl.logdet = sum(s; dims=1)\n",
        "    return cpl.mask(y_id, y_change)\n",
        "end\n",
        "#st is a neural network, the first part of the output is used as s, second part as t\n",
        "function get_s_and_t(cpl::CouplingLayer, x)\n",
        "    x_id, x_change = cpl.mask(x)\n",
        "    st = cpl.st_net(x_id)\n",
        "    middle = (size(st)[1]+1)÷2\n",
        "    s, t = st[1:middle,:], st[middle+1:end,:]\n",
        "    return (x_id, x_change, s, t)\n",
        "end\n",
        "\n",
        "\n",
        "struct RealNVP; seq::Sequential; end\n",
        "\n",
        "function RealNVP(;in_dim::Int, hidden_dim::Int, num_coupling_layers::Int, num_hidden_layers::Int)\n",
        "    coupling_layers = []\n",
        "    for i in 1:num_coupling_layers\n",
        "        push!(coupling_layers, CouplingLayer(;in_dim=in_dim, hidden_dim=hidden_dim, num_layers=num_hidden_layers, mask=Mask(div(in_dim,2), Bool(i%2))))\n",
        "    end\n",
        "    seq = Sequential(coupling_layers...)\n",
        "    RealNVP(seq)\n",
        "end\n",
        "\n",
        "(realnvp::RealNVP)(x) = realnvp.seq(x)\n",
        "\n",
        "function logdet(realNVP::RealNVP)\n",
        "    total_logdet = 0.0\n",
        "    for cpl in realNVP.seq.layers\n",
        "        total_logdet = total_logdet .+ cpl.logdet\n",
        "    end\n",
        "    return total_logdet\n",
        "end"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "logdet (generic function with 1 method)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ijXknOcrshJt",
        "colab_type": "code",
        "outputId": "93e9737e-5919-4d49-dafb-9b6dc96110e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "function make_moons_ssl()\n",
        "    Knet.seed!(2020)\n",
        "    Random.seed!(2020)\n",
        "    n_samples = 1000\n",
        "    data = MLJ.make_moons(n_samples;noise=.05,)\n",
        "    data = convert(atype, permutedims(hcat(data[1][1], data[1][2])))\n",
        "    labels = convert(atype, ones(1,n_samples)) * (-1)\n",
        "    idx1 = [1 2 4 5 6]\n",
        "    labels[idx1] = 1\n",
        "    idx0 = [3 7 8 11 18]\n",
        "    labels[idx0] = 0\n",
        "    return data, labels\n",
        "end"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "make_moons_ssl (generic function with 1 method)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0xKgo2ecIRwK",
        "colab_type": "code",
        "outputId": "7a005c7b-4874-49b9-afd7-e1a06cfcd058",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "struct Prior; means; n_components; d; gaussians; weights; end\n",
        "#n_components: number of classes\n",
        "#d: feature dimenstion of data points\n",
        "#means: d x n_components\n",
        "#gaussians: we have n_components multivariate-gaussians, each with size d\n",
        "function Prior(means)\n",
        "    d, n_components = size(means)\n",
        "    weights = convert(atype, ones(1, n_components))\n",
        "    gaussians = []\n",
        "    for i in 1:n_components\n",
        "        mu = means[:,i]\n",
        "        sig = Matrix{Float64}(I, d, d)\n",
        "        push!(gaussians, MvNormal(mu, sig))\n",
        "    end\n",
        "    Prior(means, n_components, d, gaussians, weights)\n",
        "end\n",
        "\n",
        "function log_prob(prior::Prior, z, labels=nothing; label_weight=1.0)\n",
        "    all_log_probs = []\n",
        "    for g in prior.gaussians\n",
        "        push!(all_log_probs, logpdf(g, convert(Array{Float32}, z)))\n",
        "    end\n",
        "    all_log_probs = convert(atype, hcat(all_log_probs...)) #n_instances x n_components\n",
        "    mixture_log_probs = logsumexp(all_log_probs .+ log.(softmax(prior.weights)); dims=2)\n",
        "    if labels == nothing\n",
        "        return mixture_log_probs\n",
        "    else\n",
        "        log_probs = convert(atype, zeros(size(mixture_log_probs)))\n",
        "        int_labels = convert(Array{Int32}, labels)\n",
        "        mask = (int_labels .== -1)\n",
        "        log_probs[mask] += mixture_log_probs[mask]\n",
        "        for i in 1:prior.n_components\n",
        "            mask = (int_labels .== (i-1))\n",
        "            log_probs[mask] += all_log_probs[:, i][mask] * label_weight\n",
        "        end\n",
        "        return log_probs\n",
        "    end\n",
        "end"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "log_prob (generic function with 2 methods)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lcU5YnVLsAbS",
        "colab_type": "code",
        "outputId": "bf72a4f1-e123-4efd-aea9-693fd2fe4acc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "function flow_loss(z, logdet, labels, prior; k=256)\n",
        "    prior_ll = log_prob(prior, z, labels)\n",
        "\n",
        "    #I dont know why we are doing this correction\n",
        "    batch_size = size(z,2)\n",
        "    kk = length(z) / batch_size \n",
        "    \n",
        "    corrected_prior_ll = prior_ll .- log(k) * kk\n",
        "    \n",
        "    ll = corrected_prior_ll + permutedims(logdet)\n",
        "    nll = -mean(ll)\n",
        "    return nll\n",
        "end"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "flow_loss (generic function with 1 method)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fj8bw-kFr47L",
        "colab_type": "code",
        "outputId": "1c7d2ca7-138e-46a5-f184-c236d59534fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "data, labels = make_moons_ssl()\n",
        "prior = Prior([-3.5 3.5; -3.5 3.5])\n",
        "realnvp = RealNVP(in_dim=2, hidden_dim=512, num_coupling_layers=5, num_hidden_layers=1)\n",
        "z = realnvp(data)\n",
        "sldj = logdet(realnvp)\n",
        "loss = flow_loss(z, sldj, labels, prior)"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "23.688066f0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l5vLE8S7PPYn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lr = 1e-4\n",
        "epochs = 2001\n",
        "\n",
        "num_unlabeled = Int(sum(labels .== -1))\n",
        "num_labeled = size(labels)[2] - num_unlabeled\n",
        "batch_size = num_labeled\n",
        "print_freq = 500\n",
        "\n",
        "int_labels = convert(Array{Int32}, labels)\n",
        "\n",
        "mask_labeled = [index[2] for index in findall(label->label!=-1, int_labels)]\n",
        "labeled_data = data[:,mask_labeled]\n",
        "labeled_labels = labels[mask_labeled]\n",
        "\n",
        "mask_unlabeled = [index[2] for index in findall(label->label==-1, int_labels)]\n",
        "unlabeled_data = data[:, mask_unlabeled]\n",
        "unlabeled_labels = labels[mask_unlabeled]\n",
        "\n",
        "for p in Knet.params(realnvp)\n",
        "    p.opt = Adam(;lr=lr)\n",
        "end\n",
        "\n",
        "for epoch in 1:epochs\n",
        "    batch_idx = Distributions.sample(1:num_unlabeled, batch_size, replace=false)\n",
        "    batch_x, batch_y = unlabeled_data[:, batch_idx], unlabeled_labels[batch_idx]\n",
        "    batch_x = hcat(batch_x, labeled_data)\n",
        "    batch_y = vcat(batch_y, labeled_labels)\n",
        "\n",
        "    z = realnvp(batch_x)\n",
        "    sldj = logdet(realnvp)\n",
        "    loss = @diff flow_loss(z, sldj, batch_y, prior)\n",
        "    for p in Knet.params(realnvp)\n",
        "        g = Knet.grad(loss, p)\n",
        "        update!(Knet.value(p), g, p.opt)\n",
        "    end\n",
        "    if epoch % print_freq == 0\n",
        "        print(\"iter \")\n",
        "        print(epoch)\n",
        "        print(\" loss: \")\n",
        "        print(loss)\n",
        "        println(\" \")\n",
        "    end\n",
        "    if epoch == Int(floor(epochs * 0.5)) || epoch == Int(floor(epochs * 0.8))\n",
        "        lr /= 10\n",
        "        for p in Knet.params(realnvp)\n",
        "            p.opt = Adam(;lr=lr)\n",
        "        end\n",
        "    end\n",
        "end"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Im6KjjTnYYbp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}