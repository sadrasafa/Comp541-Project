{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Warning: CuArrays.jl only supports CUDNN v7.6 or higher\n",
      "└ @ CuArrays /kuacc/users/ssafadoust20/.julia/packages/CuArrays/A6GUx/src/CuArrays.jl:122\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Knet\n",
    "# Test if Knet is using gpu\n",
    "Knet.gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Pkg;\n",
    "\n",
    "# Install missing packages\n",
    "for p in [\"Knet\", \"MLJ\", \"MLJModels\", \"Distributions\", \"Plots\"]\n",
    "    if !haskey(Pkg.installed(),p)\n",
    "        Pkg.add(p);\n",
    "    end\n",
    "end\n",
    "\n",
    "using Knet, Plots, Random,  MLJ, Distributions, LinearAlgebra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KnetArray{Float32,N} where N"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "atype = (Knet.gpu()>=0 ? Knet.KnetArray{Float32} : Array{Float32})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "logdet (generic function with 1 method)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "struct Mask; d; reverse; end\n",
    "\n",
    "#one argument: (mask) will return x_id and x_change\n",
    "#two argument: (unmask) will_return concat(y_id, y_change)\n",
    "function (mask::Mask)(x) \n",
    "    len = size(x, 1)\n",
    "    b = convert(atype,zeros(len,1))\n",
    "    d = mask.d\n",
    "    if mask.reverse \n",
    "        b[d+1:end,1] .= 1\n",
    "    else\n",
    "        b[1:d,1] .= 1\n",
    "    end\n",
    "    x_id = x .* b\n",
    "    x_change = x .* (1 .- b)\n",
    "    return x_id, x_change\n",
    "end\n",
    "function (mask::Mask)(y_id, y_change)\n",
    "    len = size(y_id, 1)\n",
    "    b = convert(atype,zeros(len,1))\n",
    "    d = mask.d\n",
    "    if mask.reverse \n",
    "        b[d+1:end,1] .= 1\n",
    "    else\n",
    "        b[1:d,1] .= 1\n",
    "    end\n",
    "    return y_id .* b + y_change .* (1 .- b)\n",
    "end\n",
    "\n",
    "\n",
    "struct Sequential\n",
    "    layers\n",
    "    Sequential(layers...) = new(layers)\n",
    "end\n",
    "(s::Sequential)(x) = (for l in s.layers; x = l(x); end; x)\n",
    "\n",
    "struct DenseLayer; w; b; f; end\n",
    "\n",
    "DenseLayer(i::Int,o::Int, f=relu) = DenseLayer(param(o,i), param0(o), f)\n",
    "\n",
    "(d::DenseLayer)(x) = d.f.(d.w * x .+ d.b)\n",
    "\n",
    "\n",
    "\n",
    "#Coupling Layer\n",
    "mutable struct CouplingLayer; st_net::Sequential; mask::Mask; logdet; end\n",
    "\n",
    "function CouplingLayer(;in_dim::Int, hidden_dim::Int, num_layers::Int, mask::Mask)\n",
    "    layers = []\n",
    "    push!(layers, DenseLayer(in_dim, hidden_dim, relu))\n",
    "    for layer in 1:num_layers\n",
    "        push!(layers, DenseLayer(hidden_dim, hidden_dim, relu))\n",
    "    end\n",
    "    push!(layers, DenseLayer(hidden_dim, 2*in_dim, identity))\n",
    "    st_net = Sequential(layers...)\n",
    "    CouplingLayer(st_net, mask, 0.0)\n",
    "end\n",
    "\n",
    "function (cpl::CouplingLayer)(x)\n",
    "    x_id, x_change, s, t = get_s_and_t(cpl, x)\n",
    "#     y_change = x_change .* exp.(s) .+ t #in original code, first addition is performed, then exponentiation\n",
    "    y_change = (x_change .+ t) .* exp.(s) \n",
    "    y_id = x_id\n",
    "    cpl.logdet = sum(s; dims=1)\n",
    "    return cpl.mask(y_id, y_change)\n",
    "end\n",
    "#st is a neural network, the first part of the output is used as s, second part as t\n",
    "function get_s_and_t(cpl::CouplingLayer, x)\n",
    "    x_id, x_change = cpl.mask(x)\n",
    "    st = cpl.st_net(x_id)\n",
    "    middle = (size(st)[1]+1)÷2\n",
    "    s, t = st[1:middle,:], st[middle+1:end,:]\n",
    "    s = tanh.(s)\n",
    "    return (x_id, x_change, s, t)\n",
    "end\n",
    "\n",
    "\n",
    "struct RealNVP; seq::Sequential; end\n",
    "\n",
    "function RealNVP(;in_dim::Int, hidden_dim::Int, num_coupling_layers::Int, num_hidden_layers::Int)\n",
    "    coupling_layers = []\n",
    "    for i in 1:num_coupling_layers\n",
    "        push!(coupling_layers, CouplingLayer(;in_dim=in_dim, hidden_dim=hidden_dim, num_layers=num_hidden_layers, mask=Mask(div(in_dim,2), Bool((i+1) %2))))\n",
    "    end\n",
    "    seq = Sequential(coupling_layers...)\n",
    "    RealNVP(seq)\n",
    "end\n",
    "\n",
    "(realnvp::RealNVP)(x) = realnvp.seq(x)\n",
    "\n",
    "function logdet(realNVP::RealNVP)\n",
    "    total_logdet = 0.0\n",
    "    for cpl in realNVP.seq.layers\n",
    "        total_logdet = total_logdet .+ cpl.logdet\n",
    "    end\n",
    "    return total_logdet\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "make_moons_ssl (generic function with 1 method)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function make_moons_ssl()\n",
    "    Knet.seed!(2020)\n",
    "    Random.seed!(2020)\n",
    "    n_samples = 1000\n",
    "    data = MLJ.make_moons(n_samples;noise=0.05)\n",
    "    data = convert(atype, permutedims(hcat(data[1][1], data[1][2])))\n",
    "    labels = convert(atype, ones(1,n_samples)) * (-1)\n",
    "    idx1 = [1 2 4 5 6]\n",
    "    labels[idx1] .= 1\n",
    "    idx0 = [3 7 8 11 18]\n",
    "    labels[idx0] .= 0\n",
    "    return data, labels\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "function mylogpdf(g,x)\n",
    "    xx = convert(Array{Float32}, Knet.value(x))\n",
    "    ans = Distributions.logpdf(g, xx)\n",
    "    return convert(atype, ans)\n",
    "end\n",
    "\n",
    "function mygradlogpdf(g,x)\n",
    "    xx = convert(Array{Float32}, Knet.value(x))\n",
    "    ans = []\n",
    "    for i in 1:size(xx,2)\n",
    "        push!(ans, Distributions.gradlogpdf(g,xx[:,i]))\n",
    "    end\n",
    "    ans = hcat(ans...)\n",
    "    return convert(atype, ans) \n",
    "end\n",
    "\n",
    "@Knet.primitive mylogpdf(g,x),dy 1 reshape(dy, (1,length(dy))).*mygradlogpdf(g,x) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "log_prob (generic function with 2 methods)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "struct Prior; means; n_components; d; gaussians; weights; end\n",
    "#n_components: number of classes\n",
    "#d: feature dimenstion of data points\n",
    "#means: d x n_components\n",
    "#gaussians: we have n_components multivariate-gaussians, each with size d\n",
    "function Prior(means)\n",
    "    d, n_components = size(means)\n",
    "    weights = convert(atype, ones(1, n_components))\n",
    "    gaussians = []\n",
    "    for i in 1:n_components\n",
    "        mu = means[:,i]\n",
    "        sig = Matrix{Float64}(I, d, d)\n",
    "        push!(gaussians, MvNormal(mu, sig))\n",
    "    end\n",
    "    Prior(means, n_components, d, gaussians, weights)\n",
    "end\n",
    "\n",
    "function log_prob(prior::Prior, z, labels=nothing; label_weight=1.0)\n",
    "    all_log_probs = []\n",
    "    for g in prior.gaussians\n",
    "        push!(all_log_probs, mylogpdf(g, z))\n",
    "    end\n",
    "    all_log_probs = hcat(all_log_probs...) #n_instances x n_components\n",
    "    mixture_log_probs = logsumexp(all_log_probs .+ log.(softmax(prior.weights)); dims=2)\n",
    "    if labels == nothing\n",
    "        return mixture_log_probs\n",
    "    else\n",
    "        #log_probs = convert(atype, zeros(size(mixture_log_probs)))\n",
    "        len = size(mixture_log_probs, 1)\n",
    "        int_labels = permutedims(convert(Array{Int32}, labels))\n",
    "        c_mixture = convert(atype, zeros(len,1))\n",
    "        mask_mixture = [index[1] for index in findall(label->label==-1, int_labels)]\n",
    "        c_mixture[mask_mixture,1] .= 1\n",
    "        log_probs = c_mixture .* mixture_log_probs\n",
    "        for i in 1:prior.n_components\n",
    "            c_all_log_probs = convert(atype, zeros(len,1))\n",
    "            mask = [index[1] for index in findall(label->label==(i-1), int_labels)]\n",
    "            c_all_log_probs[mask] .=  label_weight\n",
    "            log_probs += (c_all_log_probs .* all_log_probs[:,i:i])\n",
    "        end  \n",
    "        return log_probs\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "flow_loss (generic function with 1 method)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function flow_loss(z, logdet, labels, prior; k=256)\n",
    "    prior_ll = log_prob(prior, z, labels)\n",
    "    #I dont know why we are doing this correction\n",
    "    batch_size = size(z,2)\n",
    "    kk = length(z) / batch_size\n",
    "    \n",
    "    corrected_prior_ll = prior_ll .- log(k) * kk\n",
    "    if logdet == 0\n",
    "        ll = corrected_prior_ll\n",
    "    else\n",
    "        ll = corrected_prior_ll + permutedims(logdet)\n",
    "    end\n",
    "    nll = -mean(ll)\n",
    "    return nll\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "forward (generic function with 1 method)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function forward(realnvp, data, labels, prior)\n",
    "    z = realnvp(data)\n",
    "    sldj = logdet(realnvp)\n",
    "    return flow_loss(z, sldj, labels, prior)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T(23.626392)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data, labels = make_moons_ssl()\n",
    "prior = Prior([-3.5 3.5; -3.5 3.5])\n",
    "realnvp = RealNVP(in_dim=2, hidden_dim=512, num_coupling_layers=5, num_hidden_layers=1)\n",
    "loss = @diff forward(realnvp, data, labels, prior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 1 loss: T(25.00743) \n",
      "iter 501 loss: T(9.406581) \n",
      "iter 1001 loss: T(8.093089) \n",
      "iter 1501 loss: T(7.95645) \n",
      "iter 2001 loss: T(8.393528) \n"
     ]
    }
   ],
   "source": [
    "data, labels = make_moons_ssl()\n",
    "prior = Prior([-3.5 3.5; -3.5 3.5])\n",
    "realnvp = RealNVP(in_dim=2, hidden_dim=512, num_coupling_layers=5, num_hidden_layers=1)\n",
    "\n",
    "lr = 1e-4\n",
    "epochs = 2001\n",
    "\n",
    "num_unlabeled = Int(sum(labels .== -1))\n",
    "num_labeled = size(labels)[2] - num_unlabeled\n",
    "batch_size = num_labeled\n",
    "print_freq = 500\n",
    "\n",
    "int_labels = convert(Array{Int32}, labels)\n",
    "\n",
    "mask_labeled = [index[2] for index in findall(label->label!=-1, int_labels)]\n",
    "labeled_data = data[:,mask_labeled]\n",
    "labeled_labels = labels[mask_labeled]\n",
    "\n",
    "mask_unlabeled = [index[2] for index in findall(label->label==-1, int_labels)]\n",
    "unlabeled_data = data[:, mask_unlabeled]\n",
    "unlabeled_labels = labels[mask_unlabeled]\n",
    "\n",
    "for p in Knet.params(realnvp)\n",
    "    p.opt = Adam(;lr=lr)\n",
    "end\n",
    "for epoch in 1:epochs\n",
    "    batch_idx = Distributions.sample(1:num_unlabeled, batch_size, replace=true)\n",
    "    batch_x, batch_y = unlabeled_data[:, batch_idx], unlabeled_labels[batch_idx]\n",
    "    batch_x = hcat(batch_x, labeled_data)\n",
    "    batch_y = vcat(batch_y, labeled_labels)\n",
    "    batch_y = reshape(batch_y, (1, size(batch_y)[1]))\n",
    "\n",
    "    loss = @diff forward(realnvp, batch_x, batch_y, prior)\n",
    "     \n",
    "    for p in Knet.params(realnvp)\n",
    "        g = Knet.grad(loss, p)\n",
    "        update!(Knet.value(p), g, p.opt)\n",
    "    end\n",
    "    if epoch % print_freq == 1\n",
    "        print(\"iter \")\n",
    "        print(epoch)\n",
    "        print(\" loss: \")\n",
    "        print(loss)\n",
    "        println(\" \")\n",
    "    end\n",
    "    if epoch == Int(floor(epochs * 0.5)) || epoch == Int(floor(epochs * 0.8))\n",
    "        lr /= 10\n",
    "        for p in Knet.params(realnvp)\n",
    "            p.opt = Adam(;lr=lr)\n",
    "        end\n",
    "    end\n",
    "end"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.3.0",
   "language": "julia",
   "name": "julia-1.3"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.3.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
